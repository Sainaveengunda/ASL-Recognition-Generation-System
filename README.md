# ASL-Recognition-Generation-System
Our project contributes to Deaf and Hard of Hearing community accessibility by facilitating real-time communication and aiding education and communication tools.

**Overview:**

Our project is a holistic system for both recognizing and generating American Sign Language (ASL) gestures. Leveraging the ASL MNIST dataset, we aim to bridge communication gaps and provide a tool that benefits learners and the deaf community. Our goal is to create a system that accurately recognizes ASL gestures and generates visual ASL representations, fostering inclusive communication.

**Tasks**:

**Recognition**: Achieve multi-class image classification to identify ASL gestures accurately.
**Generation**: Synthesize visual ASL representations from textual descriptions.

**Data:**
We utilize the ASL MNIST dataset from Kaggle, comprising 27,455 training images and 7,172 test images. Each instance corresponds to a label (0-25), mapping to each alphabet letter A-Z.

**Methodology:**
**Recognition:** We evaluate CNN architectures like VGG and ResNet for image classification, focusing on accuracy and F1-score.
**Generation:** We implement Conditional Generative Adversarial Networks (cGANs) for text-to-image synthesis, using SSI and MSE for image quality assessment.

**Significance:**
Our project contributes to Deaf and Hard of Hearing community accessibility by facilitating real-time communication and aiding education and communication tools.

**Deliverables:**
A robust ASL recognition model.
A text-to-image synthesis model for generating ASL gestures.
Detailed project report.
A practical demonstration of model applications.
